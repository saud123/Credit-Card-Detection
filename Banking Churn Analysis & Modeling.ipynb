{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":358170,"sourceType":"datasetVersion","datasetId":156197}],"dockerImageVersionId":30474,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:black solid; padding: 15px; background-color: Brown; font-size:100%; text-align:left\">\n<p style=\"font-family:Georgia; font-weight:bold; letter-spacing: 2px; color:white; font-size:200%; text-align:center;padding: 0px;\"> Banking Churn Analysis & Modeling.</p></div>","metadata":{}},{"cell_type":"markdown","source":"<div align=\"center\">\n  <img src=\"https://m.economictimes.com/thumb/msid-100281493,width-1200,height-900,resizemode-4,imgsize-14062/banks-request-rbi-for-more-time-for-new-loan-provisioning-system.jpg\" alt=\"Image Description\" width=\"300\" height=\"200\">\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:black solid; padding: 15px; background-color: #FFFAF0; font-size:100%; text-align:left\">\n\n<div style=\"font-family:Georgia;background-color:'#DEB887'; padding:30px; font-size:17px\">\n\n\n\n<b>  Problem Statement:</b><br>\n    \n1. Despite the continuous efforts of banks to attract and retain customers, the banking industry faces a persistent challenge in the form of customer churn, leading to financial losses and reduced customer satisfaction.<br>\n2. It is advantageous for banks to know what leads a client towards the decision to leave the company.<br>\n    <br>\n    \n    \n<b>Project Objective:-</b> \n\n1. The aim of this project is to analyze the customer churn rate for bank because it is useful to understand why the customers leave.<br>\n2. After Analyzing we need to train a Machine Learning Model which can find the key factors that significantly influence the customer churn or attrition.<br>\n3. In the end will choose the most reliable model that will attach a probability to the churn to make it easier for customer service to target right customer in order to minimize their efforts to prevent customers churn.<br>\n<br>\n\n<b>  Project Overview:</b><br>\n\n1. <b>Churn refers</b> to customers leaving a bank or discontinuing their banking services.<br>\n2. <b>Banking Churn Analysis</b> is a process of studying customer behavior in the banking industry to predict and understand customer attrition or churn.<br>\n3. <b>Banking Churn Modeling</b> aims to identify patterns and factors that contribute to customer churn, enabling banks to take proactive measures to retain customers and improve customer satisfaction.<br></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color:##F0E68C ; font-size:100%; text-align:left\">\n\n<div style=\"font-family:Georgia;background-color:'#DEB887'; padding:30px; font-size:17px\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Table of Contents:</font></h3><br>\n\n\n1. <b>Importing Libraries & Dataset.</b><br>\n    \n    <br>\n2. <b>Data Wrangling.</b><br>\n    * Data Cleaning.<br>\n    * Handling Missing Values.<br>\n    * Handling Inconsistances.<br>\n    \n    <br>\n3. <b>Exploratory Data Analysis (EDA)</b><br>\n    * Visualizing Dependent Variable.<br>\n    * Visualizing Independent Variables.<br>\n    * Generating Insights.<br>\n    \n    <br>\n4. <b>Data Preprocessing.</b><br>\n    * Variable Selection and Importance.<br>\n    * Feature Transformation , Scaling and Encoding.<br>\n    * Splitting Data for Model Training.<br>\n    * Applying SMOTE to reduce class-imbalance.<br>\n    \n    <br>\n5. <b>Model Creation, Training and Evaluation.</b><br>\n    * Selection of Classification Algorithms.<br>\n    * Model Training and Tuning.<br>\n    * Model Evaluation and Performance.<br>\n    * Confusion Matrix Analysis.<br>\n    * Accuracy, Precision, Recall, and F1 Score.<br>\n    * Receiver Operating Characteristic (ROC) Curve and AUC.<br>\n    * Feature Importance and Contribution.<br></div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\">  Importing Required Libraries </div> ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nsns.set(style=\"darkgrid\",font_scale=1.5)\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score ,f1_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n\nfrom imblearn.over_sampling import SMOTE","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Loading Dataset</div> ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/churn-modelling/Churn_Modelling.csv\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Basic Understanding of Data</div> ","metadata":{}},{"cell_type":"markdown","source":"### 1. Checking the Dimensions of Dataset.","metadata":{}},{"cell_type":"code","source":"print(\"Total number of records/rows present in the dataset is:\",df.shape[0])\nprint(\"Total number of attributes/columns present in the dataset is:\",df.shape[1])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Fetching the Attributes Names.","metadata":{}},{"cell_type":"code","source":"df.columns","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n    \n1. **RowNumber:** It is likely a unique identifier for each record and does not contribute directly to the analysis.\n2. **CustomerId:** It can be used to track and differentiate individual customers within the dataset.\n3. **Surname:** It provides information about the family name of each customer.\n4. **CreditScore:** It is a numerical value that assesses the creditworthiness of an individual based on their credit history and financial behavior.\n5. **Geography:** It provides information about the customers' geographic distribution, allowing for analysis based on regional or national factors.\n6. **Gender:** It categorizes customers as either male or female, enabling gender-based analysis if relevant to the churn prediction.\n7. **Age:** It represents the customer's age in years and can be used to analyze age-related patterns and behaviors.\n8. **Tenure:** It typically represents the number of years or months the customer has been associated with the bank.\n9. **Balance:** It reflects the amount of money in the customer's bank account at a specific point in time.\n10. **NumOfProducts:** It can include various offerings such as savings accounts, loans, credit cards, etc.\n11. **HasCrCard:** It is a binary variable with a value of 1 if the customer possesses a credit card and 0 otherwise.\n12. **IsActiveMember:** It is a binary variable indicating whether the customer is an active member (1) or not (0) within the bank.\n13. **EstimatedSalary:** It provides an approximation of the customer's income level, which can be relevant for analyzing churn behavior.\n14. **Exited:** It indicates whether a customer has churned (1) or not (0) from the bank. It is the variable we aim to predict using the other features.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 3. Generating Basic Information of the data.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* There is total **10000 records** and **14 columns** availabe in the dataset.\n* **Out of 14 columns** there are **11 numerical columns** and **3 categorical columns.**</div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 4. Computing Total No. of Missing Values.","metadata":{}},{"cell_type":"code","source":"df.isnull().sum().to_frame().rename(columns={0:\"Total No. of Missing Values\"})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **None** of the columns is having **missing values.**\n* So we **don't** have to perform **Data Imputation.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 5. Checking Duplicate Records.","metadata":{}},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* There are **no duplicate records** present in the dataset.\n* So we can say there is no **Data Lekage** in the dataset.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 6. Performing Descriptive Statistical Analysis on Categorical Columns.","metadata":{}},{"cell_type":"code","source":"df.describe(include=\"object\").T","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Surname column** is having **very high cardinality** and **not relevant** for predicting **customer churned or not.** So we can **simply drop this feature.**\n* **Geography & Gender** Columns are having **low cardinality** and **seems relevant** for predicting **customer churned or not.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Data Wrangling</div> ","metadata":{}},{"cell_type":"markdown","source":"\n* **Data wrangling**, also known as **data munging**, refers to the process of **cleaning, transforming, and preparing raw data for analysis.** \n* It involves **handling missing values**, **addressing inconsistencies** and **formatting data** before it can be used for **further analysis**.","metadata":{}},{"cell_type":"markdown","source":"### 1. Showing Random Sample of Data.","metadata":{}},{"cell_type":"code","source":"df.sample(5)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **RowNumber** and **CustomerId** columns **represents a unique value** for each customer and **doesn't seem relevant** for predicting **customer churned or not.**\n* **Surname** column is having **high cardinality* and **doesn't seems relevant** for predicting **customer churned or not.**\n* So we can simply **drop** these features.","metadata":{}},{"cell_type":"markdown","source":"### 2. Dropping Insignificant Features.","metadata":{}},{"cell_type":"code","source":"df.drop(columns=[\"RowNumber\",\"CustomerId\",\"Surname\"],inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 3. Renaming Target Variable name and its values with more appropirate values for better Analysis.","metadata":{}},{"cell_type":"code","source":"df.rename(columns={\"Exited\":\"Churned\"},inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Churned\"].replace({0:\"No\",1:\"Yes\"},inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* We have **converted raw data** into **well-structured data** to better analysis.\n* So we can perform **Expolatory Data Analysis** and **derive insights from the data.**</div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color:black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Explorator Data Analysis</div> ","metadata":{}},{"cell_type":"markdown","source":"### 1. Visualizing Target Variable.","metadata":{}},{"cell_type":"code","source":"count = df[\"Churned\"].value_counts()\n\nplt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nax=sns.countplot(df[\"Churned\"],palette=\"Set2\")\nax.bar_label(ax.containers[0],fontweight=\"black\",size=15)\nplt.title(\"Customer Churned Disribution\",fontweight=\"black\",size=20,pad=20)\n\nplt.subplot(1,2,2)\nplt.pie(count.values, labels=count.index, autopct=\"%1.1f%%\",colors=sns.set_palette(\"Set2\"),\n        textprops={\"fontweight\":\"black\"},explode=[0,0.1])\nplt.title(\"Customer Churned Disribution\",fontweight=\"black\",size=20,pad=20)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"</div><div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* There is **huge class-imbalance** which can lead to **bias in model performance.**\n* So to **overcome** this **class-imbalance** we have to use **over-sampling technique** from **SMOTE**.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Visualizing Customer Churned by Gender.","metadata":{}},{"cell_type":"code","source":"def countplot(column):\n    plt.figure(figsize=(15,5))\n    ax = sns.countplot(x=column, data=df, hue=\"Churned\",palette=\"Set2\")\n    for value in ax.patches:\n        percentage = \"{:.1f}%\".format(100*value.get_height()/len(df[column]))\n        x = value.get_x() + value.get_width() / 2 - 0.05\n        y = value.get_y() + value.get_height()\n        ax.annotate(percentage, (x,y), fontweight=\"black\",size=15)\n        \n    plt.title(f\"Customer Churned by {column}\",fontweight=\"black\",size=20,pad=20)\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"countplot(\"Gender\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **churned probabillity** is more for **Feamle Customers** compared to **male customers**.\n* Which means **female customers** are **more deactivating their banking facilities** compared to **male customers.**\n\n    \n<h3 align=\"left\"><font color=brown>ðŸ“Š Recommendation:</font></h3>\n\n* Bank can **Develop targeted marketing campaigns** specifically tailored to **female customers.**\n* Bank can **Focus on enhancing** the overall **customer experience** for **female customers.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 3. Visualizing Customer Churned by Geoprahical Region.","metadata":{}},{"cell_type":"code","source":"countplot(\"Geography\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Half of the customers** are from **France** followed by Germany and Spain.\n* Despite of the **huge total customers difference** between France & Germany the **churned rate** for **France and Germany** customers are **same.**\n* There are **almost equal customers** from **Spain & Germany**, but the **Churn rate** is **almost double** in **Germany** when **compared with spain.**\n    \n<h3 align=\"left\"><font color=brown>ðŸ“Š Recommendation:</font></h3>\n    \n* Bank can **analyze** the **banking facilites & behaviour** of **customers from france** and try to **implement those on Germany customers.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 4. Visualizing Customer Churn by \"HasCrCard\".","metadata":{}},{"cell_type":"code","source":"countplot(\"HasCrCard\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **More than 70%** of customers are having **credit card.** \n* The **churn probability value** for the **both** the categories is **almost 1:4.**\n* So **credit card is not affecting the churn of customers.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 5. Visualizing Customer Churned by \"NumOfProducts\".","metadata":{}},{"cell_type":"code","source":"countplot(\"NumOfProducts\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Most** of the customers are having **either 1 or 2** total number of banking products.\n* **Very few** customers are having **more than 2** total number of banking products.\n* **The lowest churn rate** is with customers having **only 2 products.** \n* There is **very high churn rate** in customers having **1 product** or **more than 2 products.**\n* **Note:**\n    1. We can do **feature engineering** by **grouping the customers having products more than 2 together** to **reduce the class imbalance.**\n    2. Because **Class Imbalance** leads to **bias in model** and **misrepresentation of minority class.**\n    \n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Recommendation:</font></h3>\n\n* The **bank can try to convince the customers** to have **atleast 2 banking products.**\n* They can provide **Rewards and Incentives** to the **customers having atleast 2 banking products.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 6. Visualizing Customer Churned by \"IsActiveMember\".","metadata":{}},{"cell_type":"code","source":"countplot(\"IsActiveMember\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* There is **approxiamately equal customer** who are **either active or not active.**\n* But the **churn rate %** in **not active customers is almost double** compared to **active customers.**\n* So **customers which are not active are morely likely to deactivate their banking facilities.**\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Recommendation:</font></h3>\n    \n* Bank can provide **Regular Communication and Updates**, and **Enhanced Digital Services** so that customers remain active to the banking facilities.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 7. Visualizing Customer Churned by \"Tenure\".","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nax = sns.countplot(x=\"Tenure\", data=df, hue=\"Churned\",palette=\"Set2\")\nfor value in ax.patches:\n    percentage = \"{:.1f}%\".format(100*value.get_height()/len(df[\"Tenure\"]))\n    x = value.get_x() + value.get_width() / 2 - 0.05\n    y = value.get_y() + value.get_height()\n    ax.annotate(percentage, (x,y), fontweight=\"black\",size=12, ha=\"center\")\n\nplt.title(\"Customer Churned by Tenure\",fontweight=\"black\",size=20,pad=20)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **highest tenure is 10 years** which means that those customers have **opened their account 10 years back.**\n* Since there is **almost similar distribution of churn status** we can't make any specific inference.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 8. Visualizing Customer Churned by \"CreditScore\".","metadata":{}},{"cell_type":"code","source":"def continous_plot(column):\n    plt.figure(figsize=(13,6))\n    plt.subplot(1,2,1)\n    sns.histplot(x=column,hue=\"Churned\",data=df,kde=True,palette=\"Set2\")\n    plt.title(f\"Distribution of {column} by Churn Status\",fontweight=\"black\",pad=20,size=15)\n\n    plt.subplot(1,2,2)\n    sns.boxplot(df[\"Churned\"],df[column],palette=\"Set2\")\n    plt.title(f\"Distribution of {column} by Churn Status\",fontweight=\"black\",pad=20,size=15)\n    plt.tight_layout()\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"continous_plot(\"CreditScore\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **Median CreditScore** of **both churned** and **not churned** customers are **approxiamately equal.**\n* Since the **values are approximately equal** for both **churn status** we can't generate any **relevant inference.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 9. Visualizing Customer Churned by \"Age\".","metadata":{}},{"cell_type":"code","source":"continous_plot(\"Age\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **distribution is right skewed** because of **presence of outliers** which can lead to **overfitting in model.**\n* To **overcome this right-skewed distribution** we can use **log normal transformation** technique to bring a **normal distribution.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 10. Visualizing Customer Churned by \"Balance.\"","metadata":{}},{"cell_type":"code","source":"continous_plot(\"Balance\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **More than 3000 customers** are having their **account balance equal to zero.**\n* Customers with **zero balance** are more likely to **deactivate their account.**\n* **Excluding the zero value** we can observe a **normal distribution.** So don't have to use any other techniques.\n    \n \n * **Note:-**\n    * We can do **Feature Engineering** by **grouping** the customers with account **balance equal to 0** and **balance more than 0 separately.**\n    ","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 11. Visualizing Customer Churned by \"Estimated Salary\".","metadata":{}},{"cell_type":"code","source":"continous_plot(\"EstimatedSalary\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **median value of EstimatedSalary** is **approxiamately same** for both the **churned categories.**\n* Since the distribution is **kind of similar** for **both churn category** we can't make any **relevant inference.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Feature Engineering</div> ","metadata":{}},{"cell_type":"markdown","source":"### 1. Creating New Feature From \"NumOfProducts\" Feature.","metadata":{}},{"cell_type":"code","source":"conditions = [(df[\"NumOfProducts\"]==1), (df[\"NumOfProducts\"]==2), (df[\"NumOfProducts\"]>2)]\nvalues =     [\"One product\",\"Two Products\",\"More Than 2 Products\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Total_Products\"] = np.select(conditions,values)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=\"NumOfProducts\", inplace=True)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing The New Feature \"Total_Products\".","metadata":{}},{"cell_type":"code","source":"countplot(\"Total_Products\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Creating New Feature From \"Balance\" Feature.","metadata":{}},{"cell_type":"code","source":"conditions = [(df[\"Balance\"]==0), (df[\"Balance\"]>0)]\nvalues = [\"Zero Balance\",\"More Than zero Balance\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Account_Balance\"] = np.select(conditions, values)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.drop(columns=\"Balance\",inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualizing The New Feature \"Account_Balance\".","metadata":{}},{"cell_type":"code","source":"countplot(\"Account_Balance\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"## <div style=\"text-align: left; background-color:aliceblue ; font-family: Trebuchet MS; color: black; padding: 15px; line-height:1;border-radius:1px; margin-bottom: 0em; text-align: center; font-size: 25px;border-style: solid;border-color: dark green;\"> Data Preprocessing</div> ","metadata":{}},{"cell_type":"markdown","source":"### 1. Computing Unique Values of Categorical Columns.","metadata":{}},{"cell_type":"code","source":"cat_cols = [\"Geography\",\"Gender\",\"Total_Products\",\"Account_Balance\"]\n\nfor column in cat_cols:\n    print(f\"Unique Values in {column} column is:\",df[column].unique())\n    print(\"-\"*100,\"\\n\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Performing One Hot Encoding on Categorical Features.","metadata":{}},{"cell_type":"code","source":"df = pd.get_dummies(columns=cat_cols, data=df)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3. Encoding Target Variable.","metadata":{}},{"cell_type":"code","source":"df[\"Churned\"].replace({\"No\":0,\"Yes\":1},inplace=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 4. Checking Skewness of Continous Features.","metadata":{}},{"cell_type":"code","source":"cols = [\"CreditScore\",\"Age\",\"EstimatedSalary\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[cols].skew().to_frame().rename(columns={0:\"Feature Skewness\"})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n    \n* **Age Feature** is **highly right-skewed** , which conveys that there is presence of **positive outliers.**\n* **Skewness** can **negatively impact** the **performance** of certain machine learning algorithms, like **DecisionTree & Linear Models.**\n* To overcome the **right-skewed distribution** we can use **log normal transformation** to achieve a **normal distribution.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 4. Performing Log Transformation on Age Column.","metadata":{}},{"cell_type":"code","source":"old_age = df[\"Age\"]     ##Storing the previous Age values to compare these values with the transformed values.","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[\"Age\"] = np.log(df[\"Age\"])","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Visualizing Age Before and After Transformation.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(13,6))\nplt.subplot(1,2,1)\nsns.histplot(old_age, color=\"purple\", kde=True)\nplt.title(\"Age Distribution Before Transformation\",fontweight=\"black\",size=18,pad=20)\n\nplt.subplot(1,2,2)\nsns.histplot(df[\"Age\"], color=\"purple\", kde=True)\nplt.title(\"Age Distribution After Transformation\",fontweight=\"black\",size=18,pad=20)\nplt.tight_layout()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n    \n* After applying **log normal transformation** the **age feature has achieved** a **normal distribution.**\n* This will **help model** to find more **relevant patterns** and build a more **accurate model.** ","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 6. Segregating Features & Labels for Model Training.","metadata":{}},{"cell_type":"code","source":"X = df.drop(columns=[\"Churned\"])\ny = df[\"Churned\"]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 7. Splitting Data For Model Training & Testing.","metadata":{}},{"cell_type":"code","source":"x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Shape of x_train is:\",x_train.shape)\nprint(\"Shape of x_test is: \",x_test.shape)\nprint(\"Shape of y_train is:\",y_train.shape)\nprint(\"Shape of y_test is: \",y_test.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* Data is **equally splitted** for **Model Training & Testing.**\n* So we can build a **Predictive Model** to find the **key factors** that are significantly influencing **customers churn.**</div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"###  8. Applying SMOTE to Overcome the Class-Imbalance in Target Variable.","metadata":{}},{"cell_type":"code","source":"smt = SMOTE(random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train_resampled,y_train_resampled = smt.fit_resample(x_train,y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(x_train_resampled.shape ,y_train_resampled.shape)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_resampled.value_counts().to_frame()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Both the categories** in Target variable are now having **equal number of records.**\n* So we can **train the Model** pn **balanced records** for both churn categories and make a predictive model with **low bias.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:black solid; padding: 15px; background-color: purple; font-size:100%; text-align:left\">\n<p style=\"font-family:Georgia; font-weight:bold; letter-spacing: 2px; color:white; font-size:200%; text-align:center;padding: 0px;\"> Model Creation using DecisionTree</p></div>","metadata":{}},{"cell_type":"markdown","source":"### 1. Performing Grid-Search with cross-validation to find the best Parameters for the Model.","metadata":{}},{"cell_type":"code","source":"dtree = DecisionTreeClassifier()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\"max_depth\":[3,4,5,6,7,8,9,10],\n              \"min_samples_split\":[2,3,4,5,6,7,8],\n              \"min_samples_leaf\":[1,2,3,4,5,6,7,8],\n              \"criterion\":[\"gini\",\"entropy\"],\n              \"splitter\":[\"best\",\"random\"],\n              \"max_features\":[\"auto\",None],\n              \"random_state\":[0,42]}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid_search = GridSearchCV(dtree, param_grid, cv=5, n_jobs=-1)\n\ngrid_search.fit(x_train_resampled,y_train_resampled)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Fetching the Best Parameters for DecisionTree Model.","metadata":{}},{"cell_type":"code","source":"best_parameters = grid_search.best_params_\n\nprint(\"Best Parameters for DecisionTree Model is:\\n\\n\")\nbest_parameters","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 3. Creating DecisionTree Model Using Best Parameters.","metadata":{}},{"cell_type":"code","source":"dtree = DecisionTreeClassifier(**best_parameters)\n\ndtree.fit(x_train_resampled,y_train_resampled)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Computing Model Accuracy.","metadata":{}},{"cell_type":"code","source":"y_train_pred = dtree.predict(x_train_resampled)\ny_test_pred = dtree.predict(x_test)\n\nprint(\"Accuracy Score of Model on Training Data is =>\",round(accuracy_score(y_train_resampled,y_train_pred)*100,2),\"%\")\nprint(\"Accuracy Score of Model on Testing Data  is =>\",round(accuracy_score(y_test,y_test_pred)*100,2),\"%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Model Evaluation using Different Metric Values.","metadata":{}},{"cell_type":"code","source":"print(\"F1 Score of the Model is =>\",f1_score(y_test,y_test_pred,average=\"micro\"))\nprint(\"Recall Score of the Model is =>\",recall_score(y_test,y_test_pred,average=\"micro\"))\nprint(\"Precision Score of the Model is =>\",precision_score(y_test,y_test_pred,average=\"micro\"))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* We can observe that **recall, precision, and F1 score are all the same**, it means that our** model is achieving perfect balance between** correctly identifying **positive samples (recall)** and minimizing **false positives (precision).**\n\n    \n* The **high values** for **F1 score, recall score, and precision score**, all of which are **approximately 0.8.** These **metrics suggest** that the **model achieves good accuracy** in predicting the **positive class.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 6. Finding Importance of Features in DecisionTreeClassifier.","metadata":{}},{"cell_type":"code","source":"imp_df = pd.DataFrame({\"Feature Name\":x_train.columns,\n                       \"Importance\":dtree.feature_importances_})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = imp_df.sort_values(by=\"Importance\",ascending=False)\n\nplt.figure(figsize=(12,7))\nsns.barplot(x=\"Importance\", y=\"Feature Name\", data=features, palette=\"plasma\")\nplt.title(\"Feature Importance in the Model Prediction\", fontweight=\"black\", size=20, pad=20)\nplt.yticks(size=12)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **key factors** that significantly influence the **deactivation of customers banking facilities** are:-\n    * **`Total_Products`**, **`Age`**, **`IsActiveMember`**, **`Geography`**, **`Balance`** and **`Gender.`**\n    \n    \n* The **minimal impact** of features on the **deactivation of customers' banking facilities** are:-\n    * **`CreditScore`**, **`HasCrCard`**, **`Tenure`** and **`EstimatedSalary`**","metadata":{}},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"### 7. SHAP Summary Plot: Explaining Model Predictions with Feature Importance.","metadata":{}},{"cell_type":"code","source":"import shap\nexplainer = shap.TreeExplainer(dtree)\nshap_values = explainer.shap_values(x_test)\n\nplt.title(\"Feature Importance and Effects on Predictions\",fontweight=\"black\",pad=20,size=18)\nshap.summary_plot(shap_values[1], x_test.values, feature_names = x_test.columns,plot_size=(14,8))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **red color represents high feature values**, indicating that the **feature positively contributes** for **increasing the prediction value.**\n* The **blue color represents low feature values,** indicating that the **feature negatively contributes** for **decreasing the prediction value.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 8. Model Evaluation using Confusion Matrix.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,y_test_pred)\n\nplt.figure(figsize=(15,6))\nsns.heatmap(data=cm, linewidth=.5, annot=True, fmt=\"g\", cmap=\"Set1\")\nplt.title(\"Model Evaluation using Confusion Matrix\",fontsize=20,pad=20,fontweight=\"black\")\nplt.ylabel(\"Actual Labels\")\nplt.xlabel(\"Predicted Labels\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Strong True Positive Rate:** The model achieved a high number of true positive predictions, indicating its ability to correctly identify positive cases. This suggests that the model is effective in accurately classifying the desired outcome.\n\n    \n* **Need of Improvement in False Negative Rate:** The presence of a relatively high number of false negatives suggests that the model may have missed identifying some actual positive cases. This indicates a need for further refinement to enhance the model's ability to capture all positive cases.</div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 9. Model Evaluation: ROC Curve and Area Under the Curve (AUC)","metadata":{}},{"cell_type":"code","source":"y_pred_proba = dtree.predict_proba(x_test)[:][:,1]\n\ndf_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=[\"y_actual\"])])\ndf_actual_predicted.index = y_test.index\n\n\nfpr, tpr, thresholds = roc_curve(df_actual_predicted[\"y_actual\"], y_pred_proba)\nauc = roc_auc_score(df_actual_predicted[\"y_actual\"], y_pred_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\",color=\"green\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\",pad=20,fontweight=\"black\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n    \n1. An **AUC (Area Under the Curve) value of 0.84** suggests that the model has **strong discriminative power.** \n2. This suggests that the model has a **high ability to distinguish between positive and negative instances**, indicating its effectiveness in making accurate predictions.\n3. The **model has a relatively high probability** of ranking a randomly selected positive instance higher than a randomly selected negative instance.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:black solid; padding: 15px; background-color: purple; font-size:100%; text-align:left\">\n<p style=\"font-family:Georgia; font-weight:bold; letter-spacing: 2px; color:white; font-size:200%; text-align:center;padding: 0px;\"> Model Creation using RandomForest.</p></div>","metadata":{}},{"cell_type":"markdown","source":"### 1. Performing Grid-Search with cross-validation to find the best Parameters for the Model.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {\"max_depth\":[3,4,5,6,7,8],\n              \"min_samples_split\":[3,4,5,6,7,8],\n              \"min_samples_leaf\":[3,4,5,6,7,8],\n              \"n_estimators\": [50,70,90,100],\n              \"criterion\":[\"gini\",\"entropy\"]}","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"grid_search = GridSearchCV(rfc, param_grid, cv=5, n_jobs=-1)\n\ngrid_search.fit(x_train_resampled,y_train_resampled)","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 2. Fetching the Best Parameters for RandomForest Model.","metadata":{}},{"cell_type":"code","source":"best_parameters = grid_search.best_params_\n\nprint(\"Best Parameters for RandomForest Model is:\\n\\n\")\nbest_parameters","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 3. Creating RandomForest Model Using Best Parameters.","metadata":{}},{"cell_type":"code","source":"rfc = RandomForestClassifier(**best_parameters)\n\nrfc.fit(x_train_resampled,y_train_resampled)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4. Computing Model Accuracy.","metadata":{}},{"cell_type":"code","source":"y_train_pred = rfc.predict(x_train_resampled)\ny_test_pred  = rfc.predict(x_test)\n\nprint(\"Accuracy Score of Model on Training Data is =>\",round(accuracy_score(y_train_resampled,y_train_pred)*100,2),\"%\")\nprint(\"Accuracy Score of Model on Testing Data  is =>\",round(accuracy_score(y_test,y_test_pred)*100,2),\"%\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5. Model Evaluation using Different Metric Values.","metadata":{}},{"cell_type":"code","source":"print(\"F1 Score of the Model is =>\",f1_score(y_test,y_test_pred,average=\"micro\"))\nprint(\"Recall Score of the Model is =>\",recall_score(y_test,y_test_pred,average=\"micro\"))\nprint(\"Precision Score of the Model is =>\",precision_score(y_test,y_test_pred,average=\"micro\"))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* We can observe that **recall, precision, and F1 score are all the same**, it means that our** model is achieving perfect balance between** correctly identifying **positive samples (recall)** and minimizing **false positives (precision).**\n\n    \n* The **high values** for **F1 score, recall score, and precision score**, all of which are **approximately 0.8.** These **metrics suggest** that the **model achieves good accuracy** in predicting the **positive class.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 6. Finding Importance of Features in RandomForest Model.","metadata":{}},{"cell_type":"code","source":"imp_df = pd.DataFrame({\"Feature Name\":x_train.columns,\n                       \"Importance\":rfc.feature_importances_})","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features = imp_df.sort_values(by=\"Importance\",ascending=False)\n\nplt.figure(figsize=(12,7))\nsns.barplot(x=\"Importance\", y=\"Feature Name\", data=features, palette=\"plasma\")\nplt.title(\"Feature Importance in the Model Prediction\", fontweight=\"black\", size=20, pad=20)\nplt.yticks(size=12)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **key factors** that significantly influence the **deactivation of customers banking facilities** are:-\n    * **`Total_Products`**, **`Age`**, **`IsActiveMember`**, **`Geography`**, **`Gende`** and **`Balance`.**\n    \n    \n* The **minimal impact** of features on the **deactivation of customers' banking facilities** are:-\n    * **`HasCrCard`**, **`Tenure`**, **`CreditScore`** and **`EstimatedSalary`**","metadata":{}},{"cell_type":"markdown","source":"----","metadata":{}},{"cell_type":"markdown","source":"### 7. SHAP Summary Plot: Explaining Model Predictions with Feature Importance.","metadata":{}},{"cell_type":"code","source":"import shap\nexplainer = shap.TreeExplainer(rfc)\nshap_values = explainer.shap_values(x_test)\n\nplt.title(\"Feature Importance and Effects on Predictions\",fontweight=\"black\",pad=20,size=18)\nshap.summary_plot(shap_values[1], x_test.values, feature_names = x_test.columns,plot_size=(14,8))","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* The **red color represents high feature values**, indicating that the **feature positively contributes** for **increasing the prediction value.**\n* The **blue color represents low feature values,** indicating that the **feature negatively contributes** for **decreasing the prediction value.**","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 8. Model Evaluation using Confusion Matrix.","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(y_test,y_test_pred)\n\nplt.figure(figsize=(15,6))\nsns.heatmap(data=cm, linewidth=.5, annot=True, fmt=\"g\", cmap=\"Set1\")\nplt.title(\"Model Evaluation using Confusion Matrix\",fontsize=20,pad=20,fontweight=\"black\")\nplt.ylabel(\"Actual Labels\")\nplt.xlabel(\"Predicted Labels\")\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n* **Strong True Positive Rate:** The model achieved a high number of true positive predictions, indicating its ability to correctly identify positive cases. This suggests that the model is effective in accurately classifying the desired outcome.\n\n    \n* **Need of Improvement in False Negative Rate:** The presence of a relatively high number of false negatives suggests that the model may have missed identifying some actual positive cases. This indicates a need for further refinement to enhance the model's ability to capture all positive cases.</div>","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"### 9. Model Evaluation: ROC Curve and Area Under the Curve (AUC)","metadata":{}},{"cell_type":"code","source":"y_pred_proba = rfc.predict_proba(x_test)[:][:,1]\n\ndf_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=[\"y_actual\"])])\ndf_actual_predicted.index = y_test.index\n\n\nfpr, tpr, thresholds = roc_curve(df_actual_predicted[\"y_actual\"], y_pred_proba)\nauc = roc_auc_score(df_actual_predicted[\"y_actual\"], y_pred_proba)\n\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f\"AUC = {auc:.2f}\",color=\"green\")\nplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve\",pad=20,fontweight=\"black\")\nplt.legend()\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Inference:</font></h3>\n\n    \n1. An **AUC (Area Under the Curve) value of 0.86** suggests that the model has **strong discriminative power.** \n2. This suggests that the model has a **high ability to distinguish between positive and negative instances**, indicating its effectiveness in making accurate predictions.\n3. The **model has a relatively high probability** of ranking a randomly selected positive instance higher than a randomly selected negative instance.","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:black solid; padding: 15px; background-color: Brown; font-size:100%; text-align:left\">\n<p style=\"font-family:Georgia; font-weight:bold; letter-spacing: 2px; color:white; font-size:200%; text-align:center;padding: 0px;\"> Conclusion</p></div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; border:#808080 solid; padding: 15px; background-color: ##F0E68C ; font-size:100%; text-align:left\">\n\n<h3 align=\"left\"><font color=brown>ðŸ“Š Key-Points</font></h3>\n\n    \n* The **key factors** that significantly influence the **deactivation of customers banking facilities** are **Total_Products, Age, IsActiveMember**, **Gender** and **Geography.**\n\n    \n* **High Training and Testing Accuracies:** Both the model achieved a high accuracy score near to 90% on the training data, indicating a good fit to the training instances. Additionally, the model's accuracy score near to 85% on the testing data suggests its ability to generalize well to unseen instances.\n    \n    \n* **High F1 Score, Recall, and Precision:** The model achieved high F1 score, recall, and precision values, all approximately 0.8. This indicates that the model has a strong ability to correctly identify positive cases while minimizing false positives and maximizing true positives.\n \n \n* **High AUC value more than 0.8**, states that the model demonstrates a reasonably good discriminatory power. It suggests that the model is able to distinguish between positive and negative instances with a relatively high degree of accuracy.\n    \n    \n* **Overall Model Performance:** The model demonstrates strong performance across multiple evaluation metrics, indicating its effectiveness in making accurate predictions and capturing the desired outcomes.\n    \n    \n    \n<h3 align=\"left\"><font color=brown>ðŸ“Š Recommendations</font></h3>\n\n1. The **bank** can try to convince the customers to have **atleast 2 banking products** but **not less than 2.**\n2. The **bank** can launch a **scheme for customers with higher ages (Senior Citizens)** so that they not deactivate their **banking facilities.**\n3. The **bank** can provide **Rewards and Incentive Programs**, **Regular Communication and Updates**, and **Enhanced Digital Services** so that customers remain active to the banking facilities.","metadata":{}}]}